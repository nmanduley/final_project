{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data paths\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dirs\n",
    "# os.makedirs(POS_PATH)\n",
    "# os.makedirs(NEG_PATH)\n",
    "# os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncompress LFW database\n",
    "# # Download dataset from http://vis-www.cs.umass.edu/lfw/#download\n",
    "# # Into /facial_recognition folder\n",
    "# filename = 'lfw.tgz'\n",
    "# !tar -xf filename   # Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Put all the LFW data into the negative folder\n",
    "# for directory in os.listdir('lfw'):\n",
    "#     for file in os.listdir(os.path.join('lfw', directory)):\n",
    "#         EX_PATH = os.path.join('lfw', directory, file)\n",
    "#         NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "#         os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import uuid to generate unique identifier\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Cut frame to 250x250p\n",
    "    dim = 250\n",
    "    x_offset = 250\n",
    "    y_offset = 150\n",
    "    frame = frame[y_offset:y_offset+dim, x_offset:x_offset+dim, :]\n",
    "\n",
    "    # Collect anchors\n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        img_name = os.path.join(ANC_PATH, f'{uuid.uuid1()}.jpg')\n",
    "        cv2.imwrite(img_name, frame)\n",
    "\n",
    "    # Collect positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        img_name = os.path.join(POS_PATH, f'{uuid.uuid1()}.jpg')\n",
    "        cv2.imwrite(img_name, frame)\n",
    "\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get directories\n",
    "anchor = tf.data.Dataset.list_files(ANC_PATH + os.sep + '*.jpg').take(300)\n",
    "positive = tf.data.Dataset.list_files(POS_PATH + os.sep + '*.jpg').take(300)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH + os.sep + '*.jpg').take(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and resize\n",
    "def preprocess_image(file_path):\n",
    "    \"\"\"Receives a path to an img and returns a 100x100p normalized image\"\"\"\n",
    "    # Read image\n",
    "    raw_img = tf.io.read_file(file_path)\n",
    "\n",
    "    # Load image\n",
    "    img = tf.io.decode_jpeg(raw_img)\n",
    "\n",
    "    # Preprocessing\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    \n",
    "    # Normalizing\n",
    "    img = img/255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcatenateDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create labeled dataset\n",
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train and test partition\n",
    "def preprocess_twin(input_img, validation_img, label) -> tuple:\n",
    "    '''Receives input and validation image with the corresponding label and returns\n",
    "    a tuple containing the preprocessed input and validation image, as well as the label'''\n",
    "    return (preprocess_image(input_img), preprocess_image(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'data\\\\anchor\\\\20982ff9-856b-11ed-bce5-34e6adf636cc.jpg', b'data\\\\positive\\\\3f3121a7-856b-11ed-b610-34e6adf636cc.jpg', 1.0)\n"
     ]
    }
   ],
   "source": [
    "example = samples.next()\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = preprocess_twin(*example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloader pipeline\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partition\n",
    "train_data = data.take(round(len(data)*.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing partition\n",
    "test_data = data.skip(round(len(data)*.7))\n",
    "test_data = test_data.take(round(len(data)*0.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embedding layer\n",
    "def make_embedding():\n",
    "    inp = Input(shape=(100,100,3), name='input_image')\n",
    "    # First block of convolution-maxpooling\n",
    "    # Convolution layer\n",
    "    c1 = Conv2D(64, (10, 10), activation='relu')(inp)     # 91x91p  64 ch\n",
    "    # Maxpooling layer\n",
    "    m1 = MaxPooling2D(64, (2, 2), padding='same')(c1)     # 46x46p  64 ch\n",
    "    \n",
    "    # Second block\n",
    "    c2 = Conv2D(128, (7, 7), activation='relu')(m1)       # 40x40p  128 ch\n",
    "    m2 = MaxPooling2D(64, (2, 2), padding='same')(c2)     # 20x20p  128 ch\n",
    "    \n",
    "    # Third block\n",
    "    c3 = Conv2D(128, (4, 4), activation='relu')(m2)       # 17x17p  128 ch\n",
    "    m3 = MaxPooling2D(64, (2, 2), padding='same')(c3)     #  9x9p   128 ch\n",
    "\n",
    "    # Fourth block\n",
    "    c4 = Conv2D(256, (4, 4), activation='relu')(m3)       #  6x6p   256 ch\n",
    "    f1 = Flatten()(c4)                                    #  1xdim (dim = 6x6x256 = 9216)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)            # 4096 feature vector\n",
    "\n",
    "\n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 91, 91, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 46, 46, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 40, 40, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 20, 20, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 17, 17, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 9, 9, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build L1 distance layer\n",
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, input_embedding, validation_embedding):         # Input/anchor and pos/neg data\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    input_image = Input(name='input_img', shape=(100,100,3))\n",
    "    validation_image = Input(name='validation_img', shape=(100,100,3))\n",
    "\n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "\n",
    "    # Classification layer\n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "\n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img (InputLayer)         [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " validation_img (InputLayer)    [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)         (None, 4096)         38960448    ['input_img[0][0]',              \n",
      "                                                                  'validation_img[0][0]']         \n",
      "                                                                                                  \n",
      " distance (L1Dist)              (None, 4096)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            4097        ['distance[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss and optimizer\n",
    "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    "opt = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'siamese_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m checkpoint_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./training_checkpoints\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      3\u001b[0m checkpoint_prefix \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(checkpoint_dir, \u001b[39m'\u001b[39m\u001b[39mckpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m checkpoint \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mCheckpoint(opt\u001b[39m=\u001b[39mopt, siamese_model\u001b[39m=\u001b[39msiamese_model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'siamese_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Checkpoint callbacks\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train step function\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    # Record operations\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get anchor and positive/negative image\n",
    "        X = batch[:2]\n",
    "        # Get label\n",
    "        y = batch[2]\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = siamese_model(X, training=True)\n",
    "        # Loss\n",
    "        loss = binary_cross_loss(y, y_pred)  # True value and predicted value\n",
    "    \n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    # Updated weights and apply to siamese model\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training loop\n",
    "def train(data, EPOCHS):\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print(f'\\n Epoch {epoch}/{EPOCHS}')\n",
    "        prog_bar = tf.keras.utils.Progbar(len(data))\n",
    "\n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step\n",
    "            train_step(batch)\n",
    "            prog_bar.update(idx+1)\n",
    "        \n",
    "        # Solve checkpoints\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/50\n",
      "27/27 [==============================] - 556s 20s/step\n",
      "\n",
      " Epoch 2/50\n",
      "27/27 [==============================] - 534s 20s/step\n",
      "\n",
      " Epoch 3/50\n",
      "27/27 [==============================] - 543s 20s/step\n",
      "\n",
      " Epoch 4/50\n",
      "27/27 [==============================] - 548s 20s/step\n",
      "\n",
      " Epoch 5/50\n",
      "27/27 [==============================] - 539s 20s/step\n",
      "\n",
      " Epoch 6/50\n",
      "27/27 [==============================] - 539s 20s/step\n",
      "\n",
      " Epoch 7/50\n",
      "27/27 [==============================] - 534s 20s/step\n",
      "\n",
      " Epoch 8/50\n",
      "27/27 [==============================] - 546s 20s/step\n",
      "\n",
      " Epoch 9/50\n",
      "27/27 [==============================] - 533s 20s/step\n",
      "\n",
      " Epoch 10/50\n",
      "27/27 [==============================] - 535s 20s/step\n",
      "\n",
      " Epoch 11/50\n",
      "27/27 [==============================] - 548s 20s/step\n",
      "\n",
      " Epoch 12/50\n",
      "27/27 [==============================] - 547s 20s/step\n",
      "\n",
      " Epoch 13/50\n",
      "27/27 [==============================] - 549s 20s/step\n",
      "\n",
      " Epoch 14/50\n",
      "27/27 [==============================] - 549s 20s/step\n",
      "\n",
      " Epoch 15/50\n",
      "27/27 [==============================] - 549s 20s/step\n",
      "\n",
      " Epoch 16/50\n",
      "27/27 [==============================] - 547s 20s/step\n",
      "\n",
      " Epoch 17/50\n",
      "27/27 [==============================] - 544s 20s/step\n",
      "\n",
      " Epoch 18/50\n",
      "27/27 [==============================] - 550s 20s/step\n",
      "\n",
      " Epoch 19/50\n",
      "27/27 [==============================] - 545s 20s/step\n",
      "\n",
      " Epoch 20/50\n",
      "27/27 [==============================] - 545s 20s/step\n",
      "\n",
      " Epoch 21/50\n",
      "27/27 [==============================] - 549s 20s/step\n",
      "\n",
      " Epoch 22/50\n",
      "27/27 [==============================] - 551s 20s/step\n",
      "\n",
      " Epoch 23/50\n",
      "27/27 [==============================] - 550s 20s/step\n",
      "\n",
      " Epoch 24/50\n",
      "27/27 [==============================] - 549s 20s/step\n",
      "\n",
      " Epoch 25/50\n",
      "27/27 [==============================] - 548s 20s/step\n",
      "\n",
      " Epoch 26/50\n",
      "27/27 [==============================] - 581s 22s/step\n",
      "\n",
      " Epoch 27/50\n",
      "27/27 [==============================] - 551s 20s/step\n",
      "\n",
      " Epoch 28/50\n",
      "27/27 [==============================] - 551s 20s/step\n",
      "\n",
      " Epoch 29/50\n",
      "27/27 [==============================] - 553s 20s/step\n",
      "\n",
      " Epoch 30/50\n",
      "27/27 [==============================] - 553s 20s/step\n",
      "\n",
      " Epoch 31/50\n",
      "27/27 [==============================] - 545s 20s/step\n",
      "\n",
      " Epoch 32/50\n",
      "27/27 [==============================] - 549s 20s/step\n",
      "\n",
      " Epoch 33/50\n",
      "27/27 [==============================] - 549s 20s/step\n",
      "\n",
      " Epoch 34/50\n",
      "27/27 [==============================] - 547s 20s/step\n",
      "\n",
      " Epoch 35/50\n",
      "27/27 [==============================] - 553s 20s/step\n",
      "\n",
      " Epoch 36/50\n",
      "27/27 [==============================] - 552s 20s/step\n",
      "\n",
      " Epoch 37/50\n",
      "27/27 [==============================] - 553s 20s/step\n",
      "\n",
      " Epoch 38/50\n",
      "27/27 [==============================] - 544s 20s/step\n",
      "\n",
      " Epoch 39/50\n",
      "27/27 [==============================] - 549s 20s/step\n",
      "\n",
      " Epoch 40/50\n",
      "27/27 [==============================] - 551s 20s/step\n",
      "\n",
      " Epoch 41/50\n",
      "27/27 [==============================] - 549s 20s/step\n",
      "\n",
      " Epoch 42/50\n",
      "27/27 [==============================] - 543s 20s/step\n",
      "\n",
      " Epoch 43/50\n",
      "27/27 [==============================] - 548s 20s/step\n",
      "\n",
      " Epoch 44/50\n",
      "27/27 [==============================] - 546s 20s/step\n",
      "\n",
      " Epoch 45/50\n",
      "27/27 [==============================] - 549s 20s/step\n",
      "\n",
      " Epoch 46/50\n",
      "27/27 [==============================] - 552s 20s/step\n",
      "\n",
      " Epoch 47/50\n",
      "27/27 [==============================] - 545s 20s/step\n",
      "\n",
      " Epoch 48/50\n",
      "27/27 [==============================] - 551s 20s/step\n",
      "\n",
      " Epoch 49/50\n",
      "27/27 [==============================] - 549s 20s/step\n",
      "\n",
      " Epoch 50/50\n",
      "27/27 [==============================] - 548s 20s/step\n"
     ]
    }
   ],
   "source": [
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of data\n",
    "test_input, test_val, y_true = list(test_data.as_numpy_iterator().next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.4980272 ],\n",
       "       [0.49699104],\n",
       "       [0.4963625 ],\n",
       "       [0.499376  ],\n",
       "       [0.49924907],\n",
       "       [0.49844855],\n",
       "       [0.49627998],\n",
       "       [0.49963847],\n",
       "       [0.5000499 ],\n",
       "       [0.4974298 ],\n",
       "       [0.49962646],\n",
       "       [0.49821162],\n",
       "       [0.5000547 ],\n",
       "       [0.49861953],\n",
       "       [0.4995303 ],\n",
       "       [0.4997551 ]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = siamese_model.predict([test_input, test_val])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post processing the results\n",
    "[1 if prediction > 0.5 else 0 for prediction in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "rec = Recall()\n",
    "rec.update_state(y_true, y_pred)\n",
    "rec.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "prec = Precision()\n",
    "prec.update_state(y_true, y_pred)\n",
    "prec.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "siamese_model.save('siamese_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Reload model\n",
    "model = tf.keras.models.load_model('siamese_model.h5',\n",
    "        custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mpredict([test_input, test_val])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_input' is not defined"
     ]
    }
   ],
   "source": [
    "model.predict([test_input, test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img (InputLayer)         [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " validation_img (InputLayer)    [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)         (None, 4096)         38960448    ['input_img[0][0]',              \n",
      "                                                                  'validation_img[0][0]']         \n",
      "                                                                                                  \n",
      " l1_dist (L1Dist)               (None, 4096)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            4097        ['l1_dist[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(model, detection_threshold, verification_threshold):\n",
    "    results = []\n",
    "    for  image in os.listdir(os.path.join('app_data', 'verification_images')):\n",
    "        input_img = preprocess_image(os.path.join('app_data', 'input_images', 'input_image.jpg'))\n",
    "        validation_img = preprocess_image(os.path.join('app_data', 'verification_images', image))\n",
    "        prediction = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))\n",
    "        results.append(prediction)\n",
    "\n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    verification = detection / len(os.listdir(os.path.join('app_data', 'verification_images')))\n",
    "    verified = verification > verification_threshold\n",
    "\n",
    "    return results, verified\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV Real Time Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 298ms/step\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 251ms/step\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "1/1 [==============================] - 0s 246ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 241ms/step\n",
      "1/1 [==============================] - 0s 262ms/step\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "1/1 [==============================] - 0s 230ms/step\n",
      "1/1 [==============================] - 0s 243ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 231ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 231ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 261ms/step\n",
      "1/1 [==============================] - 0s 246ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 238ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "True\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 230ms/step\n",
      "1/1 [==============================] - 0s 265ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 237ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 246ms/step\n",
      "1/1 [==============================] - 0s 222ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 261ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 251ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 245ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 231ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "1/1 [==============================] - 0s 230ms/step\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 229ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    # Cut frame to 250x250p\n",
    "    dim = 250\n",
    "    x_offset = 250\n",
    "    y_offset = 150\n",
    "    frame = frame[y_offset:y_offset+dim, x_offset:x_offset+dim, :]\n",
    "\n",
    "    cv2.imshow('Verification', frame)\n",
    "\n",
    "    # Verification trigger\n",
    "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
    "        # Save input image to input_image folder\n",
    "        path = os.path.join('app_data', 'input_images', 'input_image.jpg')\n",
    "        cv2.imwrite(path, frame)\n",
    "        # Apply verification function\n",
    "        results, verified = verify(model, 0.5, 0.5)\n",
    "        print(verified)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.squeeze(results) > 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3c6c0ed584c41c6a98692036edee3a9ae75e8fb1758ae731ddbeee0007edfe1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
